name: Scrape BTM CSVs

on:
  workflow_dispatch:
    inputs:
      symbol:
        description: "Símbolo a scrapear"
        required: true
        default: "SPX"
      strategy:
        description: "Estrategia (Vertical o IronCondor)"
        required: true
        default: "Vertical"
      risks:
        description: "Riesgos (conservador,intermedio,agresivo,ultra_agresivo) vacío = auto"
        required: false
        default: ""
      horarios:
        description: "Horarios HH:MM separados por coma (vacío = auto)"
        required: false
        default: ""
      date_from:
        description: "YYYY-MM-DD (vacío = rango auto)"
        required: false
        default: ""
      date_to:
        description: "YYYY-MM-DD (vacío = rango auto)"
        required: false
        default: ""

permissions:
  contents: write

concurrency:
  group: scrape-btm
  cancel-in-progress: false

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    env:
      PYTHONUTF8: "1"
      TZ: America/Monterrey
      PLAYWRIGHT_BROWSERS_PATH: 0

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -c "import playwright; playwright.__dict__" || true
          python -m playwright install --with-deps chromium

      - name: Run scraper
        env:
          SYMBOL: ${{ github.event.inputs.symbol }}
          STRATEGY: ${{ github.event.inputs.strategy }}
          RIESGOS: ${{ github.event.inputs.risks }}
          HORARIOS: ${{ github.event.inputs.horarios }}
          DATE_FROM: ${{ github.event.inputs.date_from }}
          DATE_TO: ${{ github.event.inputs.date_to }}
          # URLs ya configuradas como variables de repo opcionalmente:
          DASHBOARD_URL: ${{ vars.DASHBOARD_URL }}
          LOGIN_URL: ${{ vars.LOGIN_URL }}
          BTM_EMAIL: ${{ secrets.BTM_EMAIL }}
          BTM_PASSWORD: ${{ secrets.BTM_PASSWORD }}
        run: |
          echo "Ejecutando scraper..."
          python scraper/btm_scraper.py \
            --symbol "${SYMBOL}" \
            --strategy "${STRATEGY}" \
            $( [ -n "${RIESGOS}" ] && echo --risks "${RIESGOS}" ) \
            $( [ -n "${HORARIOS}" ] && echo --horarios "${HORARIOS}" ) \
            $( [ -n "${DATE_FROM}" ] && echo --date-from "${DATE_FROM}" ) \
            $( [ -n "${DATE_TO}" ] && echo --date-to "${DATE_TO}" ) \
            --dashboard-url "${DASHBOARD_URL:-https://backtestingmarket.com/backtesting}" \
            --login-url "${LOGIN_URL:-https://backtestingmarket.com/login}" \
            --out-root "data/"

      - name: Commit & push data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Add scraped CSVs"
          file_pattern: data/**

      - name: Upload CSVs as artifact (opcional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: data-csvs
          path: data/

      # ---------- RCLONE: INSTALAR Y CONFIGURAR ----------
      - name: Install rclone
        if: always()
        run: |
          curl https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Configure rclone
        if: always()
        env:
          RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG }}
        run: |
          mkdir -p ~/.config/rclone
          printf "%s\n" "$RCLONE_CONFIG" > ~/.config/rclone/rclone.conf
          echo "Remotes disponibles:"
          rclone listremotes || true

      # ---------- SUBIR A SHAREPOINT ----------
      - name: Sync data to SharePoint (sharepoint_work:PowerBI/SPX-Scraper/)
        if: always()
        env:
          RCLONE_REMOTE_BUSINESS: sharepoint_work
        run: |
          set -e
          DEST="${RCLONE_REMOTE_BUSINESS:-sharepoint_work}:PowerBI/SPX-Scraper/"
          echo "Subiendo CSVs a: $DEST"
          rclone copy "data/" "$DEST" \
            --update --fast-list --checkers 8 --transfers 8 \
            --create-empty-src-dirs -v

      # ---------- NOTIFICACIÓN POR CORREO ----------
      - name: Send email notification
        if: always()
        uses: dawidd6/action-send-mail@v3
        with:
          # Servidor SMTP (tu proveedor)
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          secure: true
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          from: ${{ secrets.MAIL_FROM }}
          to: ${{ secrets.MAIL_TO }}
          subject: "Scrape BTM CSVs – ${{ github.event.inputs.symbol }} – ${{ job.status }}"
          convert_markdown: true
          html_body: |
            <h2>Resultado del workflow</h2>
            <ul>
              <li><b>Repo:</b> ${{ github.repository }}</li>
              <li><b>Run:</b> ${{ github.run_number }}</li>
              <li><b>Branch:</b> ${{ github.ref_name }}</li>
              <li><b>Estado:</b> <code>${{ job.status }}</code></li>
              <li><b>Símbolo:</b> ${{ github.event.inputs.symbol }}</li>
              <li><b>Estrategia:</b> ${{ github.event.inputs.strategy }}</li>
            </ul>
            <p>Los CSV se subieron a <b>SharePoint → Documentos → PowerBI → SPX-Scraper</b>.</p>
